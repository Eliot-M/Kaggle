{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fAqXHSEGZrAF",
    "outputId": "e5437e9c-479a-4322-d2b2-7c9b589048a9"
   },
   "outputs": [],
   "source": [
    "# --- Import packages --- #\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "tensorflow.test.gpu_device_name() # Checking presence of GPU device in Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "K9rJl_L5awXu",
    "outputId": "99937d6f-0604-47a0-d898-9efd2330f4ba"
   },
   "outputs": [],
   "source": [
    "# --- Connexion to Google Drive account to import data --- #\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxK4ZhhfawsW"
   },
   "outputs": [],
   "source": [
    "# --- Import datasets  --- #\n",
    "train = pd.read_csv(\"/content/drive/My Drive/Digits/train.csv\")\n",
    "test = pd.read_csv(\"/content/drive/My Drive/Digits/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLeyBH3rawu_"
   },
   "outputs": [],
   "source": [
    "# --- Shaping & Cleaning data --- #\n",
    "# Split explanatory variables from response variable\n",
    "y_train = train[\"label\"]\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# Set values from 0 - 255 to 0 - 1\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0\n",
    "\n",
    "# Reshape as 28 x 28 pixels image\n",
    "X_train = X_train.values.reshape(train.shape[0],28,28,1)\n",
    "X_sub = test.values.reshape(test.shape[0],28,28,1)\n",
    "\n",
    "# Transform to fit neural network outputs (4 = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "\n",
    "# Split in train and validation sets for learning. 42000 * 0.07 = 3000 images for validation set. \n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size = 0.07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "3FfiLwXhsdf9",
    "outputId": "c6397c68-ca94-41d4-cfc9-89a19bb2c258"
   },
   "outputs": [],
   "source": [
    "# --- Example of an image from train set --- #\n",
    "showimg = plt.imshow(X_train[0][:,:,0], cmap='Greys') # Looks like a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "n1aMgz4gsdo7",
    "outputId": "7693de1b-7897-4f12-fb6b-eadb65afa5ea"
   },
   "outputs": [],
   "source": [
    "# --- Define several CNN models : --- #\n",
    "\n",
    "# First model (Le Net 1): Conv, Conv, Max, Dropout, Conv, Conv, Max, Dropout, Flatten, Dense, Dropout, Dense\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# Second model (ResNet): Conv, Batch, Conv, Batch, Conv, Batch, Max, Dropout, Conv, Batch, Conv, Batch, Conv, Batch, Max, Dropout, Flatten, Dense, Dropout, Dense, Dropout, Dense\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (28, 28, 1)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(strides=(2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(strides=(2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Third model (Resnet variation): Standardization, Conv, Conv, Max, Batch, Conv, Conv, Max, Batch, Conv, Max, Flatten, Batch, Dense, Dense\n",
    "mean = np.mean(X_train)\n",
    "std = np.std(X_train)\n",
    "\n",
    "def standardize(x):\n",
    "    return (x-mean)/std\n",
    "\n",
    "model3=Sequential()\n",
    "\n",
    "model3.add(Lambda(standardize,input_shape=(28,28,1)))\n",
    "model3.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model3.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "    \n",
    "model3.add(MaxPool2D(pool_size=(2,2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model3.add(Conv2D(128,(3,3),activation=\"relu\"))\n",
    "    \n",
    "model3.add(MaxPool2D(pool_size=(2,2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(256,(3,3),activation=\"relu\"))\n",
    "    \n",
    "model3.add(MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "model3.add(Flatten())\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(512,activation=\"relu\"))\n",
    "model3.add(Dense(10,activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPZMTN2csdua"
   },
   "outputs": [],
   "source": [
    "# --- Generate data augmentation in batches --- #\n",
    "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
    "                            height_shift_range = 0.1,\n",
    "                            width_shift_range = 0.1,\n",
    "                            rotation_range = 20)\n",
    "\n",
    "epochs = 30 # forward-backward passes of training examples\n",
    "batch_size = 40 # Number of training examples in 1 epoch\n",
    "spe = 1000 #steps per epoch, just in case (rel. to data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RAUbIaJSsdyV",
    "outputId": "995d7374-99b4-4097-a83b-651cb4842113"
   },
   "outputs": [],
   "source": [
    "# --- Run first model --- #\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # Found on google\n",
    "annealer = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) # Reduce learning when learning stabilizes\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(X_train,Y_train, batch_size = batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch = spe, callbacks=[annealer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ksy_48GNUEs0",
    "outputId": "f1aef591-6665-428c-91ab-fdee792c2a1b"
   },
   "outputs": [],
   "source": [
    "# --- Run second model --- #\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "annealer = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model2.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "hist2 = model2.fit_generator(datagen.flow(X_train,Y_train, batch_size = batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=spe, callbacks=[annealer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wm56ugUghLLZ",
    "outputId": "f4e0ab0f-ed32-467f-c6c9-0b08c4c59bdb"
   },
   "outputs": [],
   "source": [
    "# --- Run third model --- #\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "annealer = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model3.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "hist3 = model3.fit_generator(datagen.flow(X_train,Y_train, batch_size = batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=spe, callbacks=[annealer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "-QxPUsAWNQoa",
    "outputId": "148dc594-9053-412a-c3c3-e5eda8d2144e"
   },
   "outputs": [],
   "source": [
    "# --- Results of learning (accuracy and loss) --- #\n",
    "final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))\n",
    "\n",
    "final_loss2, final_acc2 = model2.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss2, final_acc2))\n",
    "\n",
    "final_loss3, final_acc3 = model3.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss3, final_acc3))\n",
    "\n",
    "# Define the \"best model\", here based on accuracy only\n",
    "models = {'model1':final_acc, 'model2':final_acc2, 'model3': final_acc3}\n",
    "best_model = max(models.items(), key=operator.itemgetter(1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "WqSBD3-xNdIm",
    "outputId": "d0ca96f7-81da-4ae8-d3a4-0310e1871648"
   },
   "outputs": [],
   "source": [
    "# --- Print confusion matrices --- #\n",
    "y_hat = model.predict(X_val)\n",
    "y_pred = np.argmax(y_hat, axis=1)\n",
    "y_true = np.argmax(Y_val, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "y_hat2 = model2.predict(X_val)\n",
    "y_pred2 = np.argmax(y_hat2, axis=1)\n",
    "y_true = np.argmax(Y_val, axis=1)\n",
    "cm2 = confusion_matrix(y_true, y_pred2)\n",
    "print(cm2)\n",
    "\n",
    "y_hat3 = model3.predict(X_val)\n",
    "y_pred3 = np.argmax(y_hat3, axis=1)\n",
    "y_true = np.argmax(Y_val, axis=1)\n",
    "cm3 = confusion_matrix(y_true, y_pred3)\n",
    "print(cm3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors on models seems to be different. \n",
    "Ensemble models can improve predict since for each number to predict at least two models are agree (most of the time).\n",
    "It will allow to reduce errors (instead of juste choosing the 'best model').\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrTctppprxMc"
   },
   "outputs": [],
   "source": [
    "# --- Predict results with the submission file already cleanned --- #\n",
    "#y_hat = model.predict(X_sub)\n",
    "y_pred = np.argmax(model.predict(X_sub),axis=1)\n",
    "\n",
    "#y_hat2 = model2.predict(X_sub)\n",
    "y_pred2 = np.argmax(model2.predict(X_sub),axis=1)\n",
    "\n",
    "#y_hat3 = model3.predict(X_sub)\n",
    "y_pred3 = np.argmax(model3.predict(X_sub),axis=1)\n",
    "\n",
    "# Store it in pandas DataFrame\n",
    "df = pd.DataFrame({'model1':y_pred, 'model2':y_pred2, 'model3':y_pred3})\n",
    "\n",
    "# Use all model to improve predictions\n",
    "df['mode'] = df.mode(axis='columns')[0].astype(int) # most common value predicted\n",
    "\n",
    "# Use the best model if all models have different prediction otherwise use the mode.\n",
    "df['Label'] = np.where(((df['model1'] != df['model2']) & (df['model1'] != df['model3']) & (df['model3'] != df['model2']))  == True , df[best_model], df['mode'])\n",
    "# Recreate the correct index for submission\n",
    "df['ImageId'] = df.index + 1\n",
    "\n",
    "# Create the output file\n",
    "df_output = df[['ImageId', 'Label']]\n",
    "df_output.to_csv(\"/content/drive/My Drive/Digits/submission_mix.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Digits_reco.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
