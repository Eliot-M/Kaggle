{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "## Import package and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import packages --- #\n",
    "import pandas as pd \n",
    "import numpy as np #arrays and specific computations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold #ML\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression # ML models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier # ML models\n",
    "from sklearn import metrics # models parameters\n",
    "from sklearn.feature_selection import RFE, RFECV # select variables in Regression\n",
    "\n",
    "import warnings # avoid useless warning messages\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import datasets  --- #\n",
    "train_csv = pd.read_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/titanic/train.csv', index_col = 'PassengerId')\n",
    "test_csv = pd.read_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/titanic/test.csv', index_col = 'PassengerId')\n",
    "submission_csv = pd.read_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/titanic/gender_submission.csv') #Female survive only \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataviz and exploration are skipped on this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for feature engineering\n",
    "\n",
    "#### Clean NaN by median/mode imputation, create dummies and new infos from variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgUnderCondition(df, col_fill, col_condition, missing_info = False):\n",
    "    \"\"\"\n",
    "    Function to fill missing data (impute data) from median/mode values for subgroups based on other column values.\n",
    "    Important: /!\\/!\\ col_condition must not have empty values /!\\/!\\\n",
    "    \n",
    "    input:\n",
    "        df (df): data df\n",
    "        col_fill (str): column with missing values to fill\n",
    "        col_condition (list, str): List of columns to use to impute data\n",
    "        missing_info (bool): If True it add a column to specify which row had imputation.\n",
    "    \n",
    "    output: \n",
    "        df (df): input dataframe with missing values imputed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Create df with median/mode values\n",
    "    if df[col_fill].dtypes in (float,int):\n",
    "        print('Treated as a numeric : median')\n",
    "        df_for_fill = df.groupby(col_condition)[[col_fill]].median()\n",
    "        df_for_fill.reset_index(inplace = True)\n",
    "    else:\n",
    "        print('Treated as a string/object : mode')\n",
    "        df_for_fill = df.groupby(col_condition).agg(lambda x:x.value_counts().index[0])\n",
    "        df_for_fill.reset_index(inplace = True)\n",
    "        \n",
    "        allcol = col_condition.copy()\n",
    "        allcol.append(col_fill)\n",
    "        df_for_fill = df_for_fill[allcol]\n",
    "    \n",
    "    \n",
    "    # Fill NaN, from values previously computed\n",
    "    df_t = pd.merge(df, df_for_fill,  how='left', left_on=col_condition, right_on = col_condition)\n",
    "    df_t[col_fill] = np.where(pd.isnull(df_t[str(col_fill+'_x')]), df_t[str(col_fill+'_y')], df_t[str(col_fill+'_x')])\n",
    "    \n",
    "    #Add column to specify which values were missing\n",
    "    if missing_info == True:\n",
    "        df_t[str(col_fill + '_was_miss')] = np.where(pd.isnull(df_t[str(col_fill+'_x')]), 1, 0)\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    df_t.drop(columns=[str(col_fill+'_x'), str(col_fill+'_y')], inplace = True)\n",
    "    return df_t\n",
    "\n",
    "\n",
    "def addInfos(df):\n",
    "    '''\n",
    "    Not compulsary to use this as a function. Allow to do some feature engineering based on data exploration.\n",
    "    \n",
    "    input:\n",
    "        df (df): titanic df, must have 'Name', 'title', 'SibSp', 'Parch', 'Cabin', 'Age', 'Sex', 'Embarked' and 'Pclass'\n",
    "    \n",
    "    output:\n",
    "        df (df): same df with feature engineering (add columns)\n",
    "    '''\n",
    "    #Get titles from names\n",
    "    df['title'] = df.Name.str.replace('^.+, ', '', case=False)\n",
    "    df['title'] = df.title.str.replace('\\. .+', '', case=False)\n",
    "\n",
    "    df['GroupTitle'] = 'Default'\n",
    "    df['GroupTitle'][df['title'].isin(['Sir', 'Lady', 'the Countess', 'Don', 'Jonkheer'])] = 'Noble'\n",
    "    df['GroupTitle'][df['title'].isin(['Col', 'Major', 'Capt', 'Rev'])] = 'Social'\n",
    "    df['GroupTitle'][df['title'].isin(['Master'])] = 'YoungM'\n",
    "    df['GroupTitle'][df['title'].isin(['Miss', 'Mlle'])] = 'YoungF'\n",
    "    \n",
    "    #Get familly size\n",
    "    df['FamillySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['FamillyBinS'] = df['FamillySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['FamillyBinM'] = df['FamillySize'].map(lambda s: 1 if 2 <= s <= 4  else 0)\n",
    "    df['FamillyBinL'] = df['FamillySize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "    \n",
    "    #Get the cabin type (first letter of the string)\n",
    "    df['CabinType'] = df['Cabin'].map(lambda s: str(s)[0] if not pd.isnull(s) else 'No')\n",
    "    idx = df[df['CabinType'] == 'T'].index # replace the only one 'T' cabin to 'A' cabin (most similar).\n",
    "    df.loc[idx, 'CabinType'] = 'A'\n",
    "    \n",
    "    #Create new infomation about Age and Sex of passengers\n",
    "    df['AgeYoung'] = [1 if x <15 else 0 for x in list(df.Age)] #not usefull with bins of age\n",
    "    df['isMale'] = [1 if x == 'male' else 0 for x in list(df['Sex'])]\n",
    "    \n",
    "    #Ultimately merge Cabin type based on survival probability (reduce number of variables)\n",
    "    df['CabinType'] = df['CabinType'].replace(['A', 'B','C'], 'ABC')\n",
    "    df['CabinType'] = df['CabinType'].replace(['D', 'E'], 'DE')\n",
    "    df['CabinType'] = df['CabinType'].replace(['F', 'G'], 'FG')\n",
    "\n",
    "    \n",
    "    df = pd.concat([df, pd.get_dummies(df['Embarked'], prefix=\"Emb\")], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['Pclass'], prefix=\"Pc\")], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['GroupTitle'], prefix=\"Tit\")], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['CabinType'], prefix=\"Cab\")], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning, Engineering and Completion --- #\n",
    "#Prepare data for easy merge and split actions\n",
    "test = test_csv.copy()\n",
    "train = train_csv.copy()\n",
    "test['data'] = 'test'\n",
    "train['data'] = 'train'\n",
    "\n",
    "#Merge both datasets into 1 to apply same transformations\n",
    "df_all = pd.concat([train, test], sort=True).reset_index(drop=True)\n",
    "\n",
    "#\n",
    "df_all = addInfos(df_all)\n",
    "\n",
    "df_all = avgUnderCondition(df_all, 'Age', ['Pclass' , 'Sex', 'GroupTitle'], True)\n",
    "df_all = avgUnderCondition(df_all, 'Embarked', ['Pclass' , 'Sex'])\n",
    "df_all = avgUnderCondition(df_all, 'Fare', ['Pclass'])\n",
    "\n",
    "# Create some scale in quantitative values.\n",
    "df_all['FareBin'] = pd.qcut(df_all['Fare'], 9, labels= False)\n",
    "\n",
    "df_all['FareBin'] = df_all['FareBin'].replace([0, 1, 2], 0)\n",
    "df_all['FareBin'] = df_all['FareBin'].replace([3, 4], 1)\n",
    "df_all['FareBin'] = df_all['FareBin'].replace([5, 6, 7], 2)\n",
    "df_all['FareBin'] = df_all['FareBin'].replace(8, 4)\n",
    "\n",
    "df_all['AgeBin'] = pd.qcut(df_all['Age'], 7, labels= False)\n",
    "\n",
    "# Tickets bought can have an impact on survival rate (famillies, groups, etc.)\n",
    "df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "# Drop freshly useless columns\n",
    "df_all = df_all.drop(['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'title', 'GroupTitle', 'FamillySize', 'CabinType', 'Embarked', 'Fare','Age'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Re-create working dataframes --- #\n",
    "test = df_all[df_all['data'] == 'test'].drop(['Survived','data'], axis=1)\n",
    "train = df_all[df_all['data'] == 'train'].drop(['data'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predicted-Explanatory variables correlation  --- #\n",
    "df_t_corr = train.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_t_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_t_corr[df_t_corr['Feature 1'] == 'Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "#### Create dataframes from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train and validation sets --- #\n",
    "y = train[['Survived']]\n",
    "X = train.drop(['Survived'], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size= 0.1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train models --- #\n",
    "# Based on train sets, train model with cross-validation to improve predictions. Parameters are optimzed following the method in appendix\n",
    "gb_model = GradientBoostingClassifier(n_estimators=800, learning_rate=0.01, max_depth=5, min_samples_leaf= 5)\n",
    "cross_val_score(gb_model , X_train , y_train , cv=5)\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion='gini', n_estimators=2000, max_depth=6, min_samples_split=6, min_samples_leaf=5, max_features='auto') \n",
    "cross_val_score(rf_model , X_train , y_train , cv=5)\n",
    "\n",
    "\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "rgs_model = LogisticRegression()\n",
    "\n",
    "print(\"Optimal number of features: \" + str(rfecv.n_features_))\n",
    "rg_model = RFE(rgs_model, rfecv.n_features_)\n",
    "#print('Selected features: %s' % list(X.columns[rfe.support_]))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict on validation set to estimate quality of predictions --- #\n",
    "gb_model.fit(X_train , y_train)\n",
    "model_gb = list(gb_model.predict(X_val))\n",
    "\n",
    "rf_model.fit(X_train , y_train)\n",
    "model_rf = list(rf_model.predict(X_val))\n",
    "\n",
    "rg_model.fit(X_train , y_train)\n",
    "model_rg = list(rg_model.predict(X_val))\n",
    "\n",
    "y_val['pred1'] = model_gb\n",
    "y_val['pred2'] = model_rf\n",
    "y_val['pred3'] = model_rg\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Look on predictions --- #\n",
    "x = pd.crosstab(y_val.Survived, y_val.pred1)\n",
    "print(round(np.trace(x)/len(y_val)*100,1),'%')\n",
    "print(x)\n",
    "\n",
    "x = pd.crosstab(y_val.Survived, y_val.pred2)\n",
    "print(round(np.trace(x)/len(y_val)*100,1),'%')\n",
    "print(x)\n",
    "\n",
    "x = pd.crosstab(y_val.Survived, y_val.pred3)\n",
    "print(round(np.trace(x)/len(y_val)*100,1),'%')\n",
    "print(x)\n",
    "\n",
    "y = pd.crosstab(y_val.pred1, y_val.pred3)\n",
    "print(round(np.trace(y)/len(y_val)*100,1),'%')\n",
    "\n",
    "y = pd.crosstab(y_val.pred2, y_val.pred3)\n",
    "print(round(np.trace(y)/len(y_val)*100,1),'%')\n",
    "\n",
    "y = pd.crosstab(y_val.pred2, y_val.pred1)\n",
    "print(round(np.trace(y)/len(y_val)*100,1),'%')\n",
    "\n",
    "# Since models are not agree on predictions, ensemble model can help to improve predictions\n",
    "y_val['mode'] = round(y_val[['pred1', 'pred2', 'pred3']].sum(axis=1)/3,0)\n",
    "\n",
    "x = pd.crosstab(y_val.Survived , y_val['mode'])\n",
    "print(round(np.trace(x)/len(y_val)*100,1),'%')\n",
    "print(x)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict on submission file --- #\n",
    "submission = submission_csv.copy()\n",
    "\n",
    "gb_model.fit(X_train , y_train)\n",
    "model_gb = list(gb_model.predict(test))\n",
    "\n",
    "rf_model.fit(X_train , y_train)\n",
    "model_rf = list(rf_model.predict(test))\n",
    "\n",
    "rg_model.fit(X_train , y_train)\n",
    "model_rg = list(rg_model.predict(test))\n",
    "\n",
    "submission['pred1'] = model_gb\n",
    "submission['pred2'] = model_rf\n",
    "submission['pred3'] = model_rg\n",
    "\n",
    "submission = submission[['PassengerId', 'pred1', 'pred2', 'pred3']]\n",
    "submission['mode'] = submission.mode(axis='columns')[0].astype(int) # most common value predicted\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare output for Kaggle submission --- #\n",
    "out = submission[['PassengerId','mode']]\n",
    "out.columns = ['PassengerId', 'Survived']\n",
    "out.to_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/titanic/preds/submission_file.csv', index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GridSearch to search optimal parameters --- #\n",
    "\n",
    "# Example for random forest\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, oob_score = True) \n",
    "\n",
    "# Define all candidates\n",
    "grid = {\"max_depth\": [4, 6, 6, 8, 10, None],\n",
    "              \"min_samples_split\": [3, 5, 7, 10],\n",
    "              \"min_samples_leaf\": [3, 4, 5, 6],\n",
    "              \"n_estimators\" :[1000, 1500, 2000, 2500],\n",
    "              \"max_features\": ['log2', 'auto', 5, 10]\n",
    "       }\n",
    "\n",
    "# Perform GridSearch\n",
    "rfcGS = GridSearchCV(rfc,param_grid = grid , cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "\n",
    "rfcGS.fit(X_train,y_train)\n",
    "print(rfcGS.best_params_)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
